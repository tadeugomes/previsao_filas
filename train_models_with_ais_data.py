#!/usr/bin/env python3
"""
Treinar modelos reais com dados AIS coletados.

Este script:
1. Carrega complete_dataset.parquet
2. Preprocessa e adiciona features
3. Treina modelos light para cada perfil
4. Valida performance
5. Substitui modelos mock
"""

import pandas as pd
import numpy as np
from pathlib import Path
import pickle
import json
from datetime import datetime
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error

# Features para modelos light (15 features cr√≠ticas)
FEATURES_LIGHT = {
    "VEGETAL": [
        "navios_no_fundeio_na_chegada",
        "porto_tempo_medio_historico",
        "tempo_espera_ma5",
        "navios_na_fila_7d",
        "nome_porto_encoded",
        "natureza_carga_encoded",
        "movimentacao_total_toneladas",
        "mes",
        "periodo_safra",
        "dia_semana",
        "flag_soja",
        "flag_milho",
        "dwt_normalizado",
        "calado_normalizado",
        "tipo_navio_encoded",
    ],
    "MINERAL": [
        "navios_no_fundeio_na_chegada",
        "porto_tempo_medio_historico",
        "tempo_espera_ma5",
        "navios_na_fila_7d",
        "nome_porto_encoded",
        "natureza_carga_encoded",
        "movimentacao_total_toneladas",
        "mes",
        "dia_semana",
        "dwt_normalizado",
        "calado_normalizado",
        "tipo_navio_encoded",
        "densidade_carga",
        "capacidade_porto",
        "num_bercos",
    ],
    "FERTILIZANTE": [
        "navios_no_fundeio_na_chegada",
        "porto_tempo_medio_historico",
        "tempo_espera_ma5",
        "navios_na_fila_7d",
        "nome_porto_encoded",
        "natureza_carga_encoded",
        "movimentacao_total_toneladas",
        "mes",
        "periodo_safra",
        "dia_semana",
        "dwt_normalizado",
        "calado_normalizado",
        "tipo_navio_encoded",
        "flag_quimico",
        "temperatura_media",
    ],
}

# Mapeamento de tipos de carga para perfis
CARGA_TO_PROFILE = {
    "soja": "VEGETAL",
    "milho": "VEGETAL",
    "trigo": "VEGETAL",
    "farelo": "VEGETAL",
    "min√©rio": "MINERAL",
    "ferro": "MINERAL",
    "carv√£o": "MINERAL",
    "bauxita": "MINERAL",
    "fertilizante": "FERTILIZANTE",
    "ureia": "FERTILIZANTE",
    "fosfato": "FERTILIZANTE",
    "qu√≠mico": "FERTILIZANTE",
}

# Capacidades dos portos (toneladas/dia estimadas)
PORTO_CAPACIDADE = {
    "Santos": 10000,
    "Paranagu√°": 8000,
    "Rio Grande": 7000,
    "Itaqui": 5000,
    "Vit√≥ria": 6000,
    "Suape": 4000,
    "Salvador": 3000,
    "Itaja√≠": 3000,
}

PORTO_BERCOS = {
    "Santos": 15,
    "Paranagu√°": 12,
    "Rio Grande": 10,
    "Itaqui": 8,
    "Vit√≥ria": 10,
    "Suape": 8,
    "Salvador": 6,
    "Itaja√≠": 6,
}


def load_ais_data():
    """Carrega dados AIS coletados."""
    print("="*70)
    print("CARREGANDO DADOS AIS")
    print("="*70)

    df = pd.read_parquet("data/ais/complete_dataset.parquet")

    print(f"\n‚úÖ Dataset carregado: {len(df)} registros")
    print(f"   Colunas: {list(df.columns)}")
    print(f"   Per√≠odo: {df['berthing_time'].min()} a {df['berthing_time'].max()}")

    return df


def preprocess_features(df):
    """Adiciona features necess√°rias para treino."""
    print("\n" + "="*70)
    print("PREPROCESSAMENTO DE FEATURES")
    print("="*70)

    df = df.copy()

    # 1. Converter timestamps
    df['berthing_time'] = pd.to_datetime(df['berthing_time'])

    # 2. Features temporais
    print("\nüìÖ Adicionando features temporais...")
    df['mes'] = df['berthing_time'].dt.month
    df['dia_semana'] = df['berthing_time'].dt.dayofweek
    df['dia_do_ano'] = df['berthing_time'].dt.dayofyear

    # Per√≠odo de safra (soja: mar-mai e set-nov, milho: fev-jul)
    is_soja_1 = (df['mes'] >= 3) & (df['mes'] <= 5)
    is_soja_2 = (df['mes'] >= 9) & (df['mes'] <= 11)
    is_milho = (df['mes'] >= 2) & (df['mes'] <= 7)

    df['periodo_safra'] = 0
    df.loc[is_soja_1 | is_soja_2, 'periodo_safra'] = 1
    df.loc[is_milho, 'periodo_safra'] = 2

    # 3. Features de carga
    print("üì¶ Adicionando features de carga...")

    # Inferir perfil baseado no tipo de navio
    def inferir_perfil(row):
        tipo = str(row['type']).lower()
        if 'tanker' in tipo or 'chemical' in tipo:
            return 'FERTILIZANTE'
        elif 'bulk' in tipo or 'cargo' in tipo:
            # Tentar inferir baseado no porto
            porto = row['porto']
            if porto in ['Santos', 'Paranagu√°']:
                return 'VEGETAL'  # Portos agr√≠colas
            elif porto in ['Itaqui', 'Vit√≥ria']:
                return 'MINERAL'  # Portos de min√©rio
            else:
                return 'VEGETAL'  # Default
        else:
            return 'VEGETAL'

    df['perfil'] = df.apply(inferir_perfil, axis=1)

    # Flags de produto
    df['flag_soja'] = 0
    df['flag_milho'] = 0
    df['flag_quimico'] = df['type'].str.contains('chemical|tanker', case=False, na=False).astype(int)

    # Natureza da carga (inferida)
    df['natureza_carga'] = 'EXPORTACAO'  # Maioria dos portos brasileiros
    df['natureza_carga_encoded'] = 1  # 1=EXPORTACAO, 0=IMPORTACAO

    # 4. Features de porto
    print("üè≠ Adicionando features de porto...")
    df['nome_porto_encoded'] = df['porto'].astype('category').cat.codes
    df['capacidade_porto'] = df['porto'].map(PORTO_CAPACIDADE).fillna(5000)
    df['num_bercos'] = df['porto'].map(PORTO_BERCOS).fillna(8)

    # 5. Features de navio
    print("üö¢ Adicionando features de navio...")
    df['tipo_navio_encoded'] = df['type'].astype('category').cat.codes

    # DWT e calado (estimados - n√£o temos dados reais)
    # Usar valores m√©dios por tipo
    dwt_medio = {
        'cargo': 50000,
        'tanker': 60000,
        'bulk': 70000,
    }

    def estimar_dwt(tipo):
        tipo_lower = str(tipo).lower()
        for key, value in dwt_medio.items():
            if key in tipo_lower:
                return value
        return 50000

    df['dwt_normalizado'] = df['type'].apply(estimar_dwt) / 100000
    df['calado_normalizado'] = df['dwt_normalizado'] * 0.8  # Aproxima√ß√£o

    # 6. Features de movimenta√ß√£o
    print("üìä Adicionando features de movimenta√ß√£o...")
    df['movimentacao_total_toneladas'] = df['dwt_normalizado'] * 80000  # Estimativa
    df['densidade_carga'] = 1.0  # Default

    # 7. Features hist√≥ricas (rolling)
    print("üìà Calculando features hist√≥ricas...")

    # Ordenar por porto e tempo
    df = df.sort_values(['porto', 'berthing_time'])

    # Para cada porto, calcular m√©dias m√≥veis
    for porto in df['porto'].unique():
        mask = df['porto'] == porto
        df.loc[mask, 'porto_tempo_medio_historico'] = (
            df.loc[mask, 'waiting_time_hours']
            .rolling(window=10, min_periods=1)
            .mean()
        )
        df.loc[mask, 'tempo_espera_ma5'] = (
            df.loc[mask, 'waiting_time_hours']
            .rolling(window=5, min_periods=1)
            .mean()
        )

    # 8. Features de fila (estimadas)
    print("üö¶ Estimando features de fila...")

    # Contar navios na mesma janela temporal por porto
    df = df.sort_values(['porto', 'berthing_time'])

    for porto in df['porto'].unique():
        mask = df['porto'] == porto
        df_porto = df[mask].copy()

        # Para cada registro, contar quantos navios no mesmo dia
        fila_counts = []
        for idx, row in df_porto.iterrows():
            data = row['berthing_time']
            mesma_data = (
                (df_porto['berthing_time'] >= data - pd.Timedelta(days=1)) &
                (df_porto['berthing_time'] <= data + pd.Timedelta(days=1))
            )
            count = mesma_data.sum() - 1  # Exclui pr√≥prio navio
            fila_counts.append(max(0, count))

        df.loc[mask, 'navios_no_fundeio_na_chegada'] = fila_counts

    # Fila √∫ltimos 7 dias
    df['navios_na_fila_7d'] = df['navios_no_fundeio_na_chegada'] * 7  # Aproxima√ß√£o

    # 9. Features clim√°ticas (defaults)
    print("üå§Ô∏è  Adicionando features clim√°ticas (defaults)...")
    df['temperatura_media'] = 25.0  # Default Brasil
    df['precipitacao_dia'] = 0.0
    df['vento_rajada_max_dia'] = 20.0

    print(f"\n‚úÖ Preprocessamento conclu√≠do!")
    print(f"   Total de features: {len(df.columns)}")
    print(f"   Registros v√°lidos: {df['waiting_time_hours'].notna().sum()}/{len(df)}")

    return df


def train_light_model(profile, X_train, y_train, X_val, y_val, X_test, y_test):
    """Treina modelo light para um perfil."""
    print(f"\n{'='*70}")
    print(f"TREINANDO MODELO: {profile}")
    print("="*70)

    print(f"\nüìä Dados de treino:")
    print(f"   Train: {len(X_train)} amostras")
    print(f"   Val:   {len(X_val)} amostras")
    print(f"   Test:  {len(X_test)} amostras")

    # 1. LightGBM Regressor
    print(f"\nü§ñ Treinando LightGBM Regressor...")

    lgb_reg = lgb.LGBMRegressor(
        n_estimators=200,
        max_depth=8,
        learning_rate=0.05,
        num_leaves=31,
        min_child_samples=10,  # Reduzido para datasets menores
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        verbose=-1,
    )

    lgb_reg.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        eval_metric="mae",
        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]
    )

    # Predi√ß√µes
    y_train_pred = lgb_reg.predict(X_train)
    y_val_pred = lgb_reg.predict(X_val)
    y_test_pred = lgb_reg.predict(X_test)

    # M√©tricas
    train_mae = mean_absolute_error(y_train, y_train_pred)
    val_mae = mean_absolute_error(y_val, y_val_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)

    train_r2 = r2_score(y_train, y_train_pred)
    val_r2 = r2_score(y_val, y_val_pred)
    test_r2 = r2_score(y_test, y_test_pred)

    print(f"\nüìä M√©tricas Regressor:")
    print(f"   Train - MAE: {train_mae:.2f}h | R¬≤: {train_r2:.3f}")
    print(f"   Val   - MAE: {val_mae:.2f}h   | R¬≤: {val_r2:.3f}")
    print(f"   Test  - MAE: {test_mae:.2f}h  | R¬≤: {test_r2:.3f}")

    # 2. LightGBM Classifier (para categorias de tempo)
    print(f"\nü§ñ Treinando LightGBM Classifier...")

    # Categorizar tempos (0-2 dias, 2-7 dias, 7-14 dias, 14+ dias)
    def categorizar_tempo(horas):
        if horas < 48:
            return 0
        elif horas < 168:
            return 1
        elif horas < 336:
            return 2
        else:
            return 3

    y_train_cat = y_train.apply(categorizar_tempo)
    y_val_cat = y_val.apply(categorizar_tempo)
    y_test_cat = y_test.apply(categorizar_tempo)

    lgb_clf = lgb.LGBMClassifier(
        n_estimators=200,
        max_depth=8,
        learning_rate=0.05,
        num_leaves=31,
        min_child_samples=10,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        verbose=-1,
    )

    lgb_clf.fit(
        X_train, y_train_cat,
        eval_set=[(X_val, y_val_cat)],
        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]
    )

    # Acur√°cia
    train_acc = lgb_clf.score(X_train, y_train_cat)
    val_acc = lgb_clf.score(X_val, y_val_cat)
    test_acc = lgb_clf.score(X_test, y_test_cat)

    print(f"\nüìä M√©tricas Classifier:")
    print(f"   Train - Accuracy: {train_acc:.3f}")
    print(f"   Val   - Accuracy: {val_acc:.3f}")
    print(f"   Test  - Accuracy: {test_acc:.3f}")

    # 3. Feature importance
    print(f"\nüîç Top 10 features mais importantes:")
    importances = lgb_reg.feature_importances_
    feature_names = X_train.columns
    feature_imp = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)

    for idx, row in feature_imp.head(10).iterrows():
        print(f"   {row['feature']:35s}: {row['importance']:6.0f}")

    # 4. Valida√ß√£o de aceita√ß√£o
    print(f"\n‚úÖ VALIDA√á√ÉO DE ACEITA√á√ÉO:")

    criterios = {
        "MAE < 30h (test)": test_mae < 30,
        "MAE < 50h (test)": test_mae < 50,  # Crit√©rio relaxado
        "R¬≤ > 0.20 (test)": test_r2 > 0.20,  # Crit√©rio relaxado
        "Accuracy > 0.40 (test)": test_acc > 0.40,
    }

    passed = all(criterios.values())

    for criterio, passou in criterios.items():
        status = "‚úÖ" if passou else "‚ùå"
        print(f"   {status} {criterio}")

    if passed:
        print(f"\nüéâ Modelo {profile} APROVADO para produ√ß√£o!")
    else:
        print(f"\n‚ö†Ô∏è  Modelo {profile} N√ÉO atende todos os crit√©rios (mas pode ser √∫til)")

    # Retornar modelos e m√©tricas
    return {
        "lgb_reg": lgb_reg,
        "lgb_clf": lgb_clf,
        "metrics": {
            "test_mae": test_mae,
            "test_r2": test_r2,
            "test_acc": test_acc,
            "val_mae": val_mae,
            "val_r2": val_r2,
            "passed": passed,
        },
        "feature_importance": feature_imp,
    }


def save_models(profile, models, features, output_dir="models"):
    """Salva modelos treinados."""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    profile_lower = profile.lower()

    # Salvar modelos
    with open(output_path / f"{profile_lower}_light_lgb_reg.pkl", "wb") as f:
        pickle.dump(models["lgb_reg"], f)

    with open(output_path / f"{profile_lower}_light_lgb_clf.pkl", "wb") as f:
        pickle.dump(models["lgb_clf"], f)

    # Salvar metadata
    metadata = {
        "profile": profile,
        "model_type": "light",
        "is_mock": False,
        "features": features,
        "target": "tempo_espera_horas",
        "trained_at": datetime.now().isoformat(),
        "data_source": "datalastic_ais",
        "num_samples": len(models.get("X_train", [])),
        "metrics": models["metrics"],
        "artifacts": {
            "lgb_reg": f"{profile_lower}_light_lgb_reg.pkl",
            "lgb_clf": f"{profile_lower}_light_lgb_clf.pkl",
        },
        "training_params": {
            "n_estimators": 200,
            "max_depth": 8,
            "learning_rate": 0.05,
            "min_child_samples": 10,
        },
    }

    with open(output_path / f"{profile_lower}_light_metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)

    print(f"\nüíæ Modelos salvos em {output_path}/:")
    print(f"   - {profile_lower}_light_lgb_reg.pkl")
    print(f"   - {profile_lower}_light_lgb_clf.pkl")
    print(f"   - {profile_lower}_light_metadata.json")


def main():
    """Fun√ß√£o principal."""
    print("="*70)
    print("TREINO DE MODELOS REAIS COM DADOS AIS")
    print("="*70)
    print()

    # 1. Carregar dados
    df = load_ais_data()

    # 2. Preprocessar
    df = preprocess_features(df)

    # 3. Filtrar apenas registros com target v√°lido
    df_valid = df[df['waiting_time_hours'].notna()].copy()

    print(f"\n‚úÖ Dados v√°lidos para treino: {len(df_valid)}")

    # 4. Treinar modelo para cada perfil
    results = {}

    for profile in ["VEGETAL", "MINERAL", "FERTILIZANTE"]:
        # Filtrar dados do perfil
        df_profile = df_valid[df_valid['perfil'] == profile].copy()

        print(f"\n{'='*70}")
        print(f"PERFIL: {profile}")
        print("="*70)
        print(f"Amostras: {len(df_profile)}")

        if len(df_profile) < 20:
            print(f"‚ö†Ô∏è  AVISO: Poucas amostras para {profile}. Usando todos os dados.")
            df_profile = df_valid.copy()

        # Features dispon√≠veis
        available_features = [f for f in FEATURES_LIGHT[profile] if f in df_profile.columns]

        if len(available_features) < 10:
            print(f"‚ö†Ô∏è  Apenas {len(available_features)} features dispon√≠veis. Adicionando mais...")
            # Usar todas as features num√©ricas dispon√≠veis
            numeric_cols = df_profile.select_dtypes(include=[np.number]).columns
            available_features = [c for c in numeric_cols if c != 'waiting_time_hours'][:15]

        print(f"Features usadas ({len(available_features)}): {available_features}")

        # Preparar dados
        X = df_profile[available_features].fillna(0)
        y = df_profile['waiting_time_hours']

        # Split
        if len(X) >= 50:
            # Split normal
            X_temp, X_test, y_temp, y_test = train_test_split(
                X, y, test_size=0.15, random_state=42
            )
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp, test_size=0.176, random_state=42  # 0.176 * 0.85 ‚âà 0.15
            )
        else:
            # Dataset pequeno - usar propor√ß√µes menores
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            X_val, X_test, y_val, y_test = train_test_split(
                X_test, y_test, test_size=0.5, random_state=42
            )

        # Treinar
        models = train_light_model(
            profile, X_train, y_train, X_val, y_val, X_test, y_test
        )

        # Adicionar dados de treino para metadata
        models["X_train"] = X_train

        # Salvar
        save_models(profile, models, available_features)

        results[profile] = models

    # 5. Resumo final
    print("\n" + "="*70)
    print("RESUMO FINAL")
    print("="*70)

    for profile, result in results.items():
        metrics = result["metrics"]
        print(f"\n{profile}:")
        print(f"   MAE (test):  {metrics['test_mae']:.2f}h")
        print(f"   R¬≤ (test):   {metrics['test_r2']:.3f}")
        print(f"   Acc (test):  {metrics['test_acc']:.3f}")
        print(f"   Status:      {'‚úÖ APROVADO' if metrics['passed'] else '‚ö†Ô∏è  Revisar'}")

    print("\n" + "="*70)
    print("‚úÖ TREINO CONCLU√çDO!")
    print("="*70)
    print("\nModelos salvos em: models/")
    print("Pr√≥ximos passos:")
    print("  1. Testar modelos com streamlit_app.py")
    print("  2. Validar previs√µes")
    print("  3. Monitorar performance em produ√ß√£o")
    print("="*70)


if __name__ == "__main__":
    main()
